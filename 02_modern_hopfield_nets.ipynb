{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Hopfield Networks (Dense Associative Memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_data, get_samples_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete (*[Krotov and Hopfield](https://arxiv.org/abs/1606.01164)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy function:\n",
    "$$\n",
    "E = -\\sum_{i=1}^{N}F(x_{i}^{T}s)\n",
    "$$\n",
    "where $F$ is the interaction function, $x_{i}$ is the $ith$ stored pattern (one of $N$ stored patterns), $s$ is the current state.\n",
    "For example, [Demircigil et al.]() introduced the exponential interaction function:\n",
    "$$\n",
    "E = -\\sum_{i=1}^{N}\\exp(x_{i}^{T}s)\n",
    "$$\n",
    "The above equation can be rewritten as:\n",
    "$$\n",
    "E = -\\exp(lse(1,X^{T}s))\n",
    "$$\n",
    "where $X=(x_{1},...,x_{N})$ and $lse()$ is the log-sum-exp function:\n",
    "$$\n",
    "lse(l) = \\log(\\sum_{l=1}^{N}\\exp(z_{l}))\n",
    "$$\n",
    "Here, instead of having a weight matrix with stored patterns, we update the state asynchronously - component by component (for dimension $d$, the state has $d$ components denoted as $s[l]$). State's $l$-th component is updated to minimize the network energy - so the update rule depends on the difference between the current state and the current state with the $l$-th component flipped (1 -> -1 or -1 -> 1).\n",
    "$$\n",
    "s^{new}[l] = sgn[-E(s^{+})+E(s^{-})] \\\\\n",
    "\\text{rewritten as} \\\\\n",
    "s^{new}[l] = sgn[\\sum_{i=1}^{N}\\exp(x_{i}^{T}s^{+}) - \\sum_{i=1}^{N}\\exp(x_{i}^{T}s^{-})] \n",
    "$$\n",
    "where $s^{+}$ is the current state with $s[l]=1$, and $s^{-}$ is the current state with $s[l]=-1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(max_rows=50)\n",
    "X, y = get_samples_per_class(X, y)\n",
    "\n",
    "plt.imshow(np.vstack(X))\n",
    "plt.imshow(np.concatenate([x.reshape(28,28) for x in X], axis=1))\n",
    "plt.title(str(y))\n",
    "plt.show()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn to polar values and unsqueeze for ease of manipulation\n",
    "X[X < 125] = -1.\n",
    "X[X >= 125] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose patterns to store\n",
    "class_idxs_stored = [0,1,2,3,4,5,6,7,8,9]\n",
    "X_stored = X[class_idxs_stored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(z):\n",
    "    z_dim = z.shape[0] if z.shape[0] > z.shape[1] else z.shape[1]\n",
    "    c = z.max() # only for numerical stability\n",
    "    return c + np.log(np.sum([np.exp(z[l] - c) for l in range(z_dim)]))\n",
    "\n",
    "def exp_energy_func(s):\n",
    "    tmp = logsumexp(X_stored.reshape(X_stored.shape[0], X_stored.shape[1]) @ s)\n",
    "    return -np.exp(tmp)\n",
    "\n",
    "def update_state(s):\n",
    "    d = s.shape[0]\n",
    "    for l in range(d):\n",
    "        s_plus, s_minus = s.copy(), s\n",
    "        s_plus[l] = 1.\n",
    "        s_minus[l] = -1.\n",
    "        # s[l] = np.sign(-exp_energy_func(s_plus) + exp_energy_func(s_minus)) # not numerically stable\n",
    "        # exp is monotonically increasing function, hence (x > y) => (e^x > e^y)\n",
    "        s_plus_exp_args_sum = logsumexp(X_stored @ s_plus)\n",
    "        s_minus_exp_args_sum = logsumexp(X_stored @ s_minus)\n",
    "        s[l] = np.sign(s_plus_exp_args_sum - s_minus_exp_args_sum)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if stored patterns are in fact fixed points\n",
    "retrieved = []\n",
    "for c_idx in class_idxs_stored:\n",
    "    s = X_stored[c_idx].copy().reshape(-1,1)\n",
    "    retrieved.append(update_state(s).reshape(28,28))\n",
    "plt.imshow(np.concatenate(retrieved, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try retrieving random pattern\n",
    "s = np.random.choice([-1.,1.], size=(784,1))\n",
    "steps = []\n",
    "for step_i in range(3):\n",
    "    s = update_state(s)\n",
    "    steps.append(s.reshape(28,28))\n",
    "plt.imshow(np.concatenate(steps, axis=1))\n",
    "plt.title(\"Retrieval steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, in contrast to classical Hopfield Networks, the capacity is higher, pattern retrieval is much more robust - **it allows pulling apart close patterns**. In fact, the storage capacity is $C\\cong2^{\\frac{d}{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Energy function\n",
    "The energy function from the discrete state:\n",
    "$$\n",
    "E = -\\exp(lse(1,X^{T}s))\n",
    "$$\n",
    "can now be generalized to continuous-valued patterns. The new energy function is defined as:\n",
    "$$\n",
    "E = -lse(\\beta,X^{T}s) + \\frac{1}{2}s^{T}s + \\beta^{-1}\\log(N) + \\frac{1}{2}M^{2}\n",
    "$$\n",
    "where $\\beta$ is now the temperature and $M$ is the largest norm of all stored patterns. This last quadratic term with $M$ ensures that the state $s$ remains finite.\n",
    "According to the [paper by Krotov and Hopfield](https://arxiv.org/abs/2008.06996), the stored patterns $X^{T}$ can be in this scenario viewed as weights from $s$ to hidden units, while $X$ can be viewed as weights from the hidden units to $s$.\n",
    "\n",
    "##### Update rule\n",
    "The above energy function allows deriving an update rule for the state pattern $s$ by the *Concave-Convex-Procedure* described by [Yuille and Rangarajan](https://papers.nips.cc/paper/2125-the-concave-convex-procedure-cccp.pdf).\n",
    "1. The total energy $E(s)$ is split into convex and concave term: $E(s) = E_{1}(s) + E_{2}(s)$\n",
    "    * the term $E_{1}(s) = \\frac{1}{2}s^{T}s + C$ is convex ($C$ is a constant independent of $s$)\n",
    "    * the term $E_{2}(s) = -lse(\\beta,X^{T}s)$ is concave (shown in the paper)\n",
    "2. The *Concave-Convex-Procedure* applied to $E(s)$ is:\n",
    "$$\n",
    "\\nabla_{s}E_{1}(s^{t+1}) = - \\nabla_{s}E_{2}(s^{t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{s}\\left(\\frac{1}{2}s^{T}s + C \\right)(s^{t+1}) = \\nabla_{s} lse\\big(\\beta,X^{T}s^t\\big)\n",
    "$$\n",
    "$$\n",
    "s^{t+1} = X \\cdot {softmax}\\big(\\beta X^{T} s^{t} \\big)\n",
    "$$\n",
    "\n",
    "where $\\nabla_{s} lse\\big(\\beta,X^{T}s\\big) = X \\cdot {softmax}\\big(\\beta X^{T} s \\big).$\n",
    "\n",
    "Therefore, the update rule for a state pattern $s$ reads:\n",
    "$$\n",
    "s^{new} = X\\cdot{softmax}\\big(\\beta X^{T} s \\big)\n",
    "$$\n",
    "\n",
    "A few important properties *(From the paper)*:\n",
    "* The *Concave-Convex-Procedure* for obtaining the update rule guarantees monotonical decrease of the energy function\n",
    "* New energy function has global convergence to a local minimum\n",
    "* Exponential storage capacity\n",
    "* Convergence after one update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(max_rows=50)\n",
    "X, y = get_samples_per_class(X, y)\n",
    "\n",
    "plt.imshow(np.vstack(X))\n",
    "plt.imshow(np.concatenate([x.reshape(28,28) for x in X], axis=1))\n",
    "plt.title(str(y))\n",
    "plt.show()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose patterns to store\n",
    "class_idxs_stored = [0,1,2,3,4,5,6,7,8,9]\n",
    "X_stored = X[class_idxs_stored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def update_state(s, beta=1.):\n",
    "    s_new = X_stored.T @ softmax(beta * (X_stored @ s).squeeze())\n",
    "    return s_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if stored patterns are in fact fixed points\n",
    "retrieved = []\n",
    "for c_idx in class_idxs_stored:\n",
    "    s = X_stored[c_idx].copy().reshape(-1,1)\n",
    "    retrieved.append(update_state(s).reshape(28,28))\n",
    "plt.imshow(np.concatenate(retrieved, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try retrieving random pattern\n",
    "s = np.random.randn(784,1) * 255\n",
    "steps = []\n",
    "for step_i in range(3):\n",
    "    s = update_state(s)\n",
    "    steps.append(s.reshape(28,28))\n",
    "plt.imshow(np.concatenate(steps, axis=1))\n",
    "plt.title(\"Retrieval steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('hopfield')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5ee81bd40054adee790dad25ad537945a618ceb79f57194cc2ed7952e8a1a1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
